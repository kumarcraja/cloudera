Identified all hosts network ips and configured /etc/hosts as follows

172.31.1.209 node1
172.31.1.208 node2
172.31.1.207 node3
172.31.1.210 node4
172.31.1.211 node5

Also created two custom script sync configuration fie to all nodes, and run commands in all nodes.

custer_fork

[root@ip-172-31-1-209 sbin]# cat cluster_fork 
#!/bin/bash

for node in node2 node3 node4 node5
do
ssh $node $*
done

cluster_scp

#!/bin/bash

for node in node2 node3 node4 node5
do
rsync -rvlogtP $1 $node:$2
done



1) 

vm.swapiness set to 1 in all nodes

1
[root@ip-172-31-1-209 sbin]# ./cluster_fork 'cat /proc/sys/vm/swappiness'
1
1
1
1

2)

Mount attributes of disks in all nodes

[root@ip-172-31-1-209 sbin]# mount | grep xvda
/dev/xvda2 on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
[root@ip-172-31-1-209 sbin]# ./cluster_fork 'mount | grep xvda'aa
/dev/xvda2 on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
/dev/xvda2 on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
/dev/xvda2 on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
/dev/xvda2 on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
[root@ip-172-31-1-209 sbin]# 


3) 

We are using xfs file system, no reserve space for any non root usercat	

4)

transparent_hugepage disabled
=========================


[root@ip-172-31-1-209 sbin]# cat /sys/kernel/mm/transparent_hugepage/enabled 
always madvise [never]

[root@ip-172-31-1-209 sbin]# ./cluster_fork 'cat /sys/kernel/mm/transparent_hugepage/enabled'
[always] madvise never
[always] madvise never
[always] madvise never
[always] madvise never


5) 

Network interface details

eth0 172.31.1.209 node1
eth0 172.31.1.208 node2
eth0 172.31.1.207 node3
eth0 172.31.1.210 node4
eth0 172.31.1.211 node5

6) 

[root@ip-172-31-1-209 sbin]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6


172.31.1.209 ip-172-31-1-209.us-west-2.compute.internal
172.31.1.208 ip-172-31-1-208.us-west-2.compute.internal
172.31.1.207 ip-172-31-1-207.us-west-2.compute.internal
172.31.1.210 ip-172-31-1-210.us-west-2.compute.internal
172.31.1.211 ip-172-31-1-211.us-west-2.compute.internal
[root@ip-172-31-1-209 sbin]# cat /etc/resolv.conf 
# Generated by NetworkManager
search us-west-2.compute.internal
nameserver 172.31.0.2
[root@ip-172-31-1-209 sbin]# getent hosts ip-172-31-1-209.us-west-2.compute.internal
172.31.1.209    ip-172-31-1-209.us-west-2.compute.internal
[root@ip-172-31-1-209 sbin]# getent hosts 172.31.1.209
172.31.1.209    ip-172-31-1-209.us-west-2.compute.internal
[root@ip-172-31-1-209 sbin]# nslookup ip-172-31-1-209.us-west-2.compute.internal
Server:		172.31.0.2
Address:	172.31.0.2#53

Non-authoritative answer:
Name:	ip-172-31-1-209.us-west-2.compute.internal
Address: 172.31.1.209

[root@ip-172-31-1-209 sbin]# nslookup 172.31.1.209
Server:		172.31.0.2
Address:	172.31.0.2#53

Non-authoritative answer:
209.1.31.172.in-addr.arpa	name = ip-172-31-1-209.us-west-2.compute.internal.

Authoritative answers can be found from:

[root@ip-172-31-1-209 sbin]# 

7)

Verifying nscd services

[root@ip-172-31-1-209 sbin]# systemctl status nscd > /dev/null; echo $?
0
[root@ip-172-31-1-209 sbin]# ./cluster_fork 'systemctl status nscd > /dev/null; echo $?'
0
0
0
0
[root@ip-172-31-1-209 sbin]#


8)

Verifying ntpd service
=================

[root@ip-172-31-1-209 sbin]# systemctl status ntpd > /dev/null; echo $?
0
[root@ip-172-31-1-209 sbin]# ./cluster_fork 'systemctl status ntpd > /dev/null; echo $?'
0
0
0
0
[root@ip-172-31-1-209 sbin]# 





